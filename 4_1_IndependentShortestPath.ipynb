{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "conventional-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import heapq, sys, copy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-barrier",
   "metadata": {},
   "source": [
    "# Binary Bridge: Formulation and deterministic solution for shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "moderate-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryBridge:\n",
    "    \n",
    "    def __init__(self, n_stages, mu0, sig02):\n",
    "        \"\"\"\n",
    "        아래와 같이 생긴 이진 그래프에서 최단 거리 탐색 문제 해결(예시: n_stages=4):\n",
    "                                    o\n",
    "                                  /   \\\n",
    "                                o - o - o\n",
    "                              /   /   /   \\\n",
    "                        시작 o - o - o - o - o 끝\n",
    "               stage number 0   1   2   3   4\n",
    "\n",
    "         - 시작부터 끝까지 가는 최단경로 탐색\n",
    "         - 각 edge의 길이를 평균과 분산이 각각 mu0, sig02인 로그정규분포에서 생성\n",
    "        \"\"\"\n",
    "        assert (n_stages % 2 == 0), 'number of stages has to be even'\n",
    "        self.n_stages = n_stages\n",
    "        self.mu0 = mu0\n",
    "        self.sig02 = sig02\n",
    "        \n",
    "        self.nodes = set() # graph의 key들(node들)을 생성\n",
    "        self.graph = defaultdict(dict) # 각 node에 연결된 node의 edge에 해당하는 theta를 생성\n",
    "        self.minimum_distance = None\n",
    "        self.shortest_path = None\n",
    "        \n",
    "        self._create_graph()\n",
    "        self._prev = defaultdict(list)\n",
    "        self._apply_dijkstra()\n",
    "        \n",
    "    def _get_stage_width(self, stage):\n",
    "        \"\"\"\n",
    "        stage별 노드 개수를 반환하는 method(ex. 0->1, 1->2, 2->3, 3->2, 4->1)\n",
    "        \"\"\"\n",
    "        width = stage - 2 * max(stage - self.n_stages/2, 0) + 1\n",
    "        return int(width)\n",
    "    \n",
    "    def _generate_nodes(self):\n",
    "        \"\"\"\n",
    "        노드들을 순서쌍 형태로 생성\n",
    "        \"\"\"\n",
    "        for x in range(self.n_stages + 1):\n",
    "            for y in range(self._get_stage_width(x)):\n",
    "                self.nodes.add((x, y))\n",
    "        \n",
    "    def _generate_edges(self):\n",
    "        \"\"\"\n",
    "        로그정규분포에서 노드들 사이의 edge에 해당하는 theta를 생성\n",
    "        \"\"\"\n",
    "        for x in range(self.n_stages + 1):\n",
    "            for y in range(self._get_stage_width(x)):\n",
    "                current = (x, y)\n",
    "                up_shape = (x+1, y+1)\n",
    "                flat_shape = (x+1, y)\n",
    "                down_shape = (x+1, y-1)\n",
    "                \n",
    "                if up_shape in self.nodes:\n",
    "                    self.graph[current][up_shape] = np.exp(np.random.normal(self.mu0, np.sqrt(self.sig02)))\n",
    "                if flat_shape in self.nodes:\n",
    "                    self.graph[current][flat_shape] = np.exp(np.random.normal(self.mu0, np.sqrt(self.sig02)))\n",
    "                if down_shape in self.nodes and flat_shape not in self.nodes:\n",
    "                    self.graph[current][down_shape] = np.exp(np.random.normal(self.mu0, np.sqrt(self.sig02)))\n",
    "\n",
    "                \n",
    "    def _create_graph(self):\n",
    "        \"\"\"\n",
    "        노드와 엣지를 생성해 그래프를 정의함\n",
    "        \"\"\"\n",
    "        self._generate_nodes()\n",
    "        self._generate_edges()\n",
    "        \n",
    "    def _apply_dijkstra(self):\n",
    "        \"\"\"\n",
    "        Dijkstra 알고리즘을 사용해 현재 생성된 그래프 내의 최단 거리를 계산하고 최단 경로를 저장\n",
    "        \"\"\"\n",
    "        distances = dict([(node, np.inf) for node in self.nodes])\n",
    "        distances[(0,0)] = 0\n",
    "        prev_path = defaultdict(list)\n",
    "\n",
    "        to_visit = [[0, (0,0)]]\n",
    "        while to_visit:\n",
    "            distance, visiting = heapq.heappop(to_visit)\n",
    "            for destination in self.graph[visiting].keys():\n",
    "                new_distance = distance + self.graph[visiting][destination]\n",
    "                if new_distance < distances[destination]:\n",
    "                    distances[destination] = new_distance\n",
    "                    prev_path[destination] = prev_path[visiting] + [visiting]\n",
    "                    heapq.heappush(to_visit, [new_distance, destination])\n",
    "        \n",
    "        arrival = (self.n_stages, 0)\n",
    "        self.minimum_distance = distances[arrival]\n",
    "        self.shortest_path = prev_path[arrival] + [arrival]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-container",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "opening-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(BinaryBridge):\n",
    "    \n",
    "    def overwrite_edge_length(self, edge_length):\n",
    "        for start_node in edge_length:\n",
    "            for end_node in edge_length[start_node]:\n",
    "                self.graph[start_node][end_node] = edge_length[start_node][end_node]\n",
    "                \n",
    "    def get_shortest_path(self):\n",
    "        self._apply_dijkstra()\n",
    "        return self.shortest_path\n",
    "    \n",
    "    def generate_outcome(self, action, sig2_tilde):\n",
    "        \"\"\"\n",
    "        action  : list of nodes\n",
    "        outcome : dictionary of elapsed time({start : {end : generated_elapse_time}})\n",
    "        \"\"\"\n",
    "        elapsed_times = defaultdict(dict)\n",
    "        for visiting, destination in zip(action, action[1:]):\n",
    "            theta = self.graph[visiting][destination]\n",
    "            parameter_ln = np.log(theta) - sig2_tilde / 2\n",
    "            elapsed_time = np.exp(np.random.normal(parameter_ln, sig2_tilde))\n",
    "            elapsed_times[visiting][destination] = elapsed_time\n",
    "\n",
    "        return elapsed_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-stadium",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "binary-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    \n",
    "    def __init__(self, n_stages, mu0, sig02, sig2_tilde=1., epsilon=0.0):\n",
    "        assert (n_stages % 2 == 0), 'number of stages has to be even'\n",
    "        self.n_stages = n_stages\n",
    "        self.mu0 = mu0\n",
    "        self.sig02 = sig02\n",
    "        self.sig2_tilde = sig2_tilde\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.internal_env = Environment(n_stages, mu0, sig02)\n",
    "        self.posterior_params = copy.deepcopy(self.internal_env.graph)\n",
    "        for visiting in self.posterior_params:\n",
    "            for destination in self.posterior_params[visiting]:\n",
    "                self.posterior_params[visiting][destination] = (mu0, sig02)\n",
    "    \n",
    "    def update_parameters(self, action, reward):\n",
    "        \n",
    "        for visiting in reward:\n",
    "            for destination in reward[visiting]:\n",
    "                elapse_time = reward[visiting][destination]\n",
    "                previous_mean, previous_var = self.posterior_params[visiting][destination]\n",
    "                \n",
    "                precision_theta = 1. / previous_var\n",
    "                precision_noise = 1. / self.sig2_tilde\n",
    "                updated_var = 1. / (precision_theta + precision_noise)\n",
    "                \n",
    "                updated_mean = precision_theta * previous_mean + precision_noise * (np.log(elapse_time) + self.sig2_tilde / 2)\n",
    "                updated_mean = updated_mean / (precision_theta + precision_noise)\n",
    "                \n",
    "                self.posterior_params[visiting][destination] = (updated_mean, updated_var)\n",
    "        \n",
    "    def _posterior_mean(self):\n",
    "        posterior_means = copy.deepcopy(self.posterior_params)\n",
    "        for visiting in self.posterior_params:\n",
    "            for destination in self.posterior_params[visiting]:\n",
    "                mean, var = self.posterior_params[visiting][destination]\n",
    "                posterior_means[visiting][destination] = np.exp(mean + var / 2)\n",
    "        \n",
    "        return posterior_means\n",
    "    \n",
    "    def _explore(self):\n",
    "        path = []\n",
    "        start_node = (0,0)\n",
    "        while True:\n",
    "            path += [start_node]\n",
    "            if start_node == (self.n_stages, 0):\n",
    "                break\n",
    "            start_node = random.choice(list(self.internal_env.graph.keys()))\n",
    "        return path\n",
    "    \n",
    "    def _exploit(self):\n",
    "        posterior_means = self._posterior_mean()\n",
    "        self.internal_env.overwrite_edge_length(posterior_means)\n",
    "        return self.internal_env.get_shortest_path()\n",
    "    \n",
    "    def pick_action(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            path = self._explore()\n",
    "        else:\n",
    "            path = self._exploit()\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "infinite-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Thompson(EpsilonGreedy):\n",
    "    \n",
    "    def _posterior_sample(self):\n",
    "        posterior_samples = copy.deepcopy(self.posterior_params)\n",
    "        for visiting in self.posterior_params:\n",
    "            for destination in self.posterior_params[visiting]:\n",
    "                mean, var = self.posterior_params[visiting][destination]\n",
    "                posterior_samples[visiting][destination] = np.exp(np.random.normal(mean, var))\n",
    "        return posterior_samples\n",
    "        \n",
    "    def pick_action(self):\n",
    "        posterior_samples = self._posterior_sample()\n",
    "        self.internal_env.overwrite_edge_length(posterior_samples)\n",
    "        return self.internal_env.get_shortest_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-punishment",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "introductory-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(response, aggregate=False):\n",
    "    if aggregate:\n",
    "        reward = 0\n",
    "        for visiting in response:\n",
    "            for destination in response[visiting]:\n",
    "                reward += response[visiting][destination]\n",
    "    else:\n",
    "        reward = response\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "green-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "\n",
    "    def __init__(self, agent, environment, n_steps, exp_id):\n",
    "        \"\"\"\n",
    "        agent       : predefined instance of Greedy or Thompson class\n",
    "        environment : predefined instance of Environment class\n",
    "        n_steps     : number of steps in current experiment\n",
    "        exp_id      : id of current experiment\n",
    "        \"\"\"\n",
    "        self.agent = agent\n",
    "        self.environment = environment\n",
    "        self.optimal_reward = -environment.minimum_distance\n",
    "        self.n_steps = n_steps\n",
    "        self.exp_id = exp_id\n",
    "        self.result = []\n",
    "        self.data_dict = {}\n",
    "\n",
    "    def _step(self, step_index):\n",
    "        # pick action -> generate outcome -> observe reward of the action -> update belief accordingly\n",
    "        action = self.agent.pick_action()\n",
    "        response = self.environment.generate_outcome(action, self.agent.sig2_tilde)\n",
    "        self.agent.update_parameters(action, reward_function(response))\n",
    "\n",
    "        # calculate regret of current action\n",
    "        expected_reward = 0\n",
    "        for visiting, destination in zip(action, action[1:]):\n",
    "            expected_reward -= self.environment.graph[visiting][destination]\n",
    "        regret = self.optimal_reward - expected_reward\n",
    "\n",
    "        # Leave log\n",
    "        self.cum_regret += regret\n",
    "        self.data_dict = {'step': (step_index + 1), \n",
    "                          'regret': regret, \n",
    "                          'total_dist': reward_function(response, True), \n",
    "                          'action': action, \n",
    "                          'experiment_id':self.exp_id}\n",
    "        self.result.append(self.data_dict)\n",
    "    \n",
    "    def run(self):\n",
    "        self.cum_regret = 0\n",
    "        for t in range(self.n_steps):\n",
    "            self._step(t)\n",
    "        self.result = pd.DataFrame(self.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "disabled-disclosure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "n_stages = 20\n",
    "mu0 = -0.5\n",
    "sig02 = 1\n",
    "\n",
    "n_steps = 500\n",
    "n_experiment = 100\n",
    "environment = Environment(n_stages, mu0, sig02)\n",
    "\n",
    "for exp_id in range(1, n_experiment+1):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(str(exp_id))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    np.random.seed(exp_id)\n",
    "    for agent_type in ['greedy', 'ts']:\n",
    "        agent = EpsilonGreedy(n_stages, mu0, sig02) if agent_type == 'greedy' else Thompson(n_stages, mu0, sig02)\n",
    "        experiment = Experiment(agent, environment, n_steps, exp_id)\n",
    "        experiment.run()\n",
    "        experiment.result.insert(experiment.result.shape[1], 'agent', agent_type)\n",
    "        results.append(experiment.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-weekly",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
